# # lstm_pytorch_trainer.py

# import os
# import torch
# import numpy as np
# import matplotlib.pyplot as plt
# import matplotlib
# import matplotlib.dates as mdates
# from matplotlib import font_manager
# from torch import nn
# from torch.utils.data import DataLoader, TensorDataset

# from utils.bybit_api import get_bybit_historical_data
# from utils.preprocess import preprocess_lstm_data
# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
# from sklearn.metrics import mean_squared_error

# from models.lstm_model import LSTMRegressor  # âœ… ì™¸ë¶€ ëª¨ë¸ íŒŒì¼ì—ì„œ import

# # âœ… í•œê¸€ í°íŠ¸ ì„¤ì • (ë°°í¬ í™˜ê²½ì—ì„œë„ ì•ˆì „í•˜ê²Œ)
# try:
#     font_path = "assets/micross.ttf"
#     if os.path.exists(font_path):
#         font_prop = font_manager.FontProperties(fname=font_path)
#         font_name = font_prop.get_name()
#         matplotlib.rcParams['font.family'] = font_name
#     else:
#         font_prop = None
# except Exception as e:
#     print(f"âš ï¸ [LOG] í°íŠ¸ ì„¤ì • ì‹¤íŒ¨: {e}")
#     font_prop = None

# matplotlib.rcParams['axes.unicode_minus'] = False

# # í•˜ì´í¼íŒŒë¼ë¯¸í„°
# SEQ_LEN = 60
# BATCH_SIZE = 32
# EPOCHS = 30
# LR = 0.001

# def train_and_save_model(interval_name, bybit_interval):
#     print(f"\nğŸ“ˆ [{interval_name}] ë°ì´í„°ë¡œ í•™ìŠµ ì‹œì‘")

#     # 1. ë°ì´í„° ìˆ˜ì§‘
#     df = get_bybit_historical_data(interval=bybit_interval, limit=1000)
#     if df is None or df.empty:
#         print(f"âŒ [{interval_name}] ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
#         return

#     # 2. ì „ì²˜ë¦¬
#     X_train, y_train, X_test, y_test, scaler = preprocess_lstm_data(df, sequence_length=SEQ_LEN)

#     # y_test shape ë³´ì • (scaler ì˜¤ë¥˜ ë°©ì§€)
#     if y_test.ndim == 1:
#         y_test = y_test.reshape(-1, 1)

#     # 3. Tensor ë³€í™˜
#     X_train = torch.tensor(X_train, dtype=torch.float32)
#     y_train = torch.tensor(y_train, dtype=torch.float32)
#     train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)

#     # 4. ëª¨ë¸ ì •ì˜
#     model = LSTMRegressor()
#     print(model)  # â­â­â­â­â­ ëª¨ë¸ êµ¬ì¡° ì¶œë ¥
#     criterion = nn.MSELoss()
#     optimizer = torch.optim.Adam(model.parameters(), lr=LR)

#     # 5. í•™ìŠµ
#     model.train()
#     for epoch in range(EPOCHS):
#         total_loss = 0
#         for batch_X, batch_y in train_loader:
#             optimizer.zero_grad()
#             output = model(batch_X)
#             loss = criterion(output, batch_y)
#             loss.backward()
#             optimizer.step()
#             total_loss += loss.item()
#         print(f"[{interval_name}] Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss:.4f}")

#     # 6. ëª¨ë¸ ì €ì¥
#     os.makedirs("models", exist_ok=True)
#     model_path = f"models/lstm_model_{interval_name}.pt"
#     torch.save(model.state_dict(), model_path)
#     print(f"âœ… [{interval_name}] ëª¨ë¸ ì €ì¥ ì™„ë£Œ â†’ {model_path}")

#     # 7. ì •í™•ë„ í‰ê°€
#     model.eval()
#     with torch.no_grad():
#         y_pred = model(torch.tensor(X_test, dtype=torch.float32)).numpy()

#     y_pred_real = scaler.inverse_transform(y_pred)
#     y_test_real = scaler.inverse_transform(y_test)

#     mae = mean_absolute_error(y_test_real, y_pred_real)
#     rmse = mean_squared_error(y_test_real, y_pred_real) ** 0.5  # ìˆ˜ë™ìœ¼ë¡œ ë£¨íŠ¸ ê³„ì‚°
#     r2 = r2_score(y_test_real, y_pred_real)

#     print(f"\nğŸ“Š [{interval_name}] ì •í™•ë„ í‰ê°€ ê²°ê³¼")
#     print(f"ğŸ“‰ MAE: {mae:.2f}")
#     print(f"ğŸ“‰ RMSE: {rmse:.2f}")
#     print(f"ğŸ“ˆ RÂ² Score: {r2:.4f}")

#     # 8. ì‹œê°í™” (ì„ íƒì ìœ¼ë¡œ)
#     try:
#         fig, ax = plt.subplots(figsize=(12, 5))
#         ax.plot(df.index[-100:], df['close'][-100:], label='ì‹¤ì œ ê°€ê²©', linewidth=2)
#         ax.plot(df.index[-1], y_pred_real[-1], 'ro', label='ì˜ˆì¸¡ ê°€ê²©')
#         ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))
#         fig.autofmt_xdate()
#         ax.set_title(f'{interval_name} ì˜ˆì¸¡ ê²°ê³¼', fontsize=14, fontproperties=font_prop)
#         ax.set_xlabel('ì‹œê°„', fontproperties=font_prop)
#         ax.set_ylabel('ê°€ê²© (USDT)', fontproperties=font_prop)
#         ax.legend(prop=font_prop)
#         ax.grid(True)
#         plt.tight_layout()
#         plt.show()
#     except Exception as vis_err:
#         print(f"âš ï¸ [LOG] ì‹œê°í™” ì‹¤íŒ¨: {vis_err}")

# # ğŸ”¥ ë©”ì¸ ì‹¤í–‰ë¶€
# if __name__ == "__main__":
#     print("ğŸ”¥ FILE EXECUTED\n")

#     INTERVALS = {
#         '5m': '5',
#         '15m': '15',
#         '1h': '60',
#         '4h': '240'
#     }

#     for name, bybit_code in INTERVALS.items():
#         train_and_save_model(name, bybit_code)

# lstm_pytorch_trainer.py

import os
import torch
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import matplotlib.dates as mdates
from matplotlib import font_manager
from torch import nn
from torch.utils.data import DataLoader, TensorDataset

from utils.bybit_api import get_bybit_historical_data
from utils.preprocess import preprocess_lstm_data
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.metrics import mean_squared_error

from models.lstm_model import LSTMRegressor  # âœ… ì™¸ë¶€ ëª¨ë¸ íŒŒì¼ì—ì„œ import

# âœ… í•œê¸€ í°íŠ¸ ì„¤ì • (ë°°í¬ í™˜ê²½ì—ì„œë„ ì•ˆì „í•˜ê²Œ)
try:
    font_path = "assets/micross.ttf"
    if os.path.exists(font_path):
        font_prop = font_manager.FontProperties(fname=font_path)
        font_name = font_prop.get_name()
        matplotlib.rcParams['font.family'] = font_name
    else:
        font_prop = None
except Exception as e:
    print(f"âš ï¸ [LOG] í°íŠ¸ ì„¤ì • ì‹¤íŒ¨: {e}")
    font_prop = None

matplotlib.rcParams['axes.unicode_minus'] = False

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
SEQ_LEN = 60
BATCH_SIZE = 32
EPOCHS = 30
LR = 0.001

def train_and_save_model(interval_name, bybit_interval):
    print(f"\nğŸ“ˆ [{interval_name}] ë°ì´í„°ë¡œ í•™ìŠµ ì‹œì‘")

    # 1. ë°ì´í„° ìˆ˜ì§‘ - Bybit API v5 í˜•ì‹ì— ë§ê²Œ interval ìˆ˜ì •
    INTERVALS = {
        '5m': '5',
        '15m': '15',
        '1h': '60',
        '4h': '240'
    }
    
    df = get_bybit_historical_data(interval=INTERVALS[interval_name], limit=1000)
    if df is None or df.empty:
        print(f"âŒ [{interval_name}] ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
        return

    # 2. ì „ì²˜ë¦¬
    X_train, y_train, X_test, y_test, scaler = preprocess_lstm_data(df, sequence_length=SEQ_LEN)

    # y_test shape ë³´ì • (scaler ì˜¤ë¥˜ ë°©ì§€)
    if y_test.ndim == 1:
        y_test = y_test.reshape(-1, 1)

    # 3. Tensor ë³€í™˜
    X_train = torch.tensor(X_train, dtype=torch.float32)
    y_train = torch.tensor(y_train, dtype=torch.float32)
    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)

    # 4. ëª¨ë¸ ì •ì˜
    model = LSTMRegressor()
    print(model)  # â­â­â­â­â­ ëª¨ë¸ êµ¬ì¡° ì¶œë ¥
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)

    # 5. í•™ìŠµ
    model.train()
    for epoch in range(EPOCHS):
        total_loss = 0
        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            output = model(batch_X)
            loss = criterion(output, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"[{interval_name}] Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss:.4f}")

    # 6. ëª¨ë¸ ì €ì¥
    os.makedirs("models", exist_ok=True)
    model_path = f"models/lstm_model_{interval_name}.pt"
    torch.save(model.state_dict(), model_path)
    print(f"âœ… [{interval_name}] ëª¨ë¸ ì €ì¥ ì™„ë£Œ â†’ {model_path}")

    # 7. ì •í™•ë„ í‰ê°€
    model.eval()
    with torch.no_grad():
        y_pred = model(torch.tensor(X_test, dtype=torch.float32)).numpy()

    y_pred_real = scaler.inverse_transform(y_pred)
    y_test_real = scaler.inverse_transform(y_test)

    mae = mean_absolute_error(y_test_real, y_pred_real)
    rmse = mean_squared_error(y_test_real, y_pred_real) ** 0.5  # ìˆ˜ë™ìœ¼ë¡œ ë£¨íŠ¸ ê³„ì‚°
    r2 = r2_score(y_test_real, y_pred_real)

    print(f"\nğŸ“Š [{interval_name}] ì •í™•ë„ í‰ê°€ ê²°ê³¼")
    print(f"ğŸ“‰ MAE: {mae:.2f}")
    print(f"ğŸ“‰ RMSE: {rmse:.2f}")
    print(f"ğŸ“ˆ RÂ² Score: {r2:.4f}")

    # 8. ì‹œê°í™” (ì„ íƒì ìœ¼ë¡œ)
    try:
        fig, ax = plt.subplots(figsize=(12, 5))
        ax.plot(df.index[-100:], df['close'][-100:], label='ì‹¤ì œ ê°€ê²©', linewidth=2)
        ax.plot(df.index[-1], y_pred_real[-1], 'ro', label='ì˜ˆì¸¡ ê°€ê²©')
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))
        fig.autofmt_xdate()
        ax.set_title(f'{interval_name} ì˜ˆì¸¡ ê²°ê³¼', fontsize=14, fontproperties=font_prop)
        ax.set_xlabel('ì‹œê°„', fontproperties=font_prop)
        ax.set_ylabel('ê°€ê²© (USDT)', fontproperties=font_prop)
        ax.legend(prop=font_prop)
        ax.grid(True)
        plt.tight_layout()
        plt.show()
    except Exception as vis_err:
        print(f"âš ï¸ [LOG] ì‹œê°í™” ì‹¤íŒ¨: {vis_err}")

# ğŸ”¥ ë©”ì¸ ì‹¤í–‰ë¶€
if __name__ == "__main__":
    print("ğŸ”¥ FILE EXECUTED\n")

    INTERVALS = {
        '5m': '5',
        '15m': '15',
        '1h': '60',
        '4h': '240'
    }

    for name, bybit_code in INTERVALS.items():
        train_and_save_model(name, bybit_code)
