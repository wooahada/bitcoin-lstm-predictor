# train_lstm_multi.py

import os
import torch
import torch.nn as nn
import numpy as np
from torch.utils.data import TensorDataset, DataLoader

from utils.bybit_api import get_bybit_historical_data
from utils.preprocess import preprocess_lstm_data
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

from models.lstm_model import LSTMRegressor  # âœ… ëª¨ë¸ í´ë˜ìŠ¤ë§Œ import

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
SEQ_LEN = 60
BATCH_SIZE = 32
EPOCHS = 30
LR = 0.001
INTERVALS = {
    '5m': '5',
    '15m': '15',
    '1h': '60',
    '4h': '240'
}

class LSTMRegressor(nn.Module):
    def __init__(self, input_size=1, hidden_size=50, num_layers=2):
        super(LSTMRegressor, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :])

def train_and_save_model(interval_name, bybit_interval):
        print(f"\nğŸš€ [{interval_name}] í•™ìŠµ ì‹œì‘ - API interval: {bybit_interval}")

        try:
            df = get_bybit_historical_data(interval=bybit_interval, limit=1000)
            print(f"âœ… [{interval_name}] ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ - shape: {df.shape}")
        except Exception as e:
            print(f"âŒ [{interval_name}] ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return

        print(f"\nğŸ“ˆ [{interval_name}] ë°ì´í„°ë¡œ í•™ìŠµ ì‹œì‘")

        # 1. ë°ì´í„° ìˆ˜ì§‘
        df = get_bybit_historical_data(interval=bybit_interval, limit=1000)

        # 2. ì „ì²˜ë¦¬
        X_train, y_train, X_test, y_test, scaler = preprocess_lstm_data(df, sequence_length=SEQ_LEN)

        # 3. Tensor ë³€í™˜
        X_train = torch.tensor(X_train, dtype=torch.float32)
        y_train = torch.tensor(y_train, dtype=torch.float32)
        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)

        # 4. ëª¨ë¸ ì •ì˜
        model = LSTMRegressor()
        criterion = nn.MSELoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=LR)

        # 5. í•™ìŠµ
        model.train()
        for epoch in range(EPOCHS):
            total_loss = 0
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                output = model(batch_X)
                loss = criterion(output, batch_y)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            print(f"[{interval_name}] Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss:.4f}")

        # 6. ëª¨ë¸ ì €ì¥
        os.makedirs("models", exist_ok=True)
        model_path = f"models/lstm_model_{interval_name}.pt"
        torch.save(model.state_dict(), model_path)
        print(f"âœ… [{interval_name}] ëª¨ë¸ ì €ì¥ ì™„ë£Œ â†’ {model_path}")

        # âœ… 7. ì •í™•ë„ í‰ê°€ (í•¨ìˆ˜ ë‚´ë¶€!)
        model.eval()
        with torch.no_grad():
            y_pred = model(torch.tensor(X_test, dtype=torch.float32)).numpy()

        y_pred_real = scaler.inverse_transform(y_pred)
        y_test_real = scaler.inverse_transform(y_test)

        mae = mean_absolute_error(y_test_real, y_pred_real)
        rmse = np.sqrt(mean_squared_error(y_test_real, y_pred_real))  # ğŸ”§ ìˆ˜ì •ëœ ì¤„
        r2 = r2_score(y_test_real, y_pred_real)


        print(f"\nğŸ“Š [{interval_name}] ì •í™•ë„ í‰ê°€ ê²°ê³¼")
        print(f"ğŸ“‰ MAE (ì ˆëŒ€ ì˜¤ì°¨ í‰ê· ): {mae:.2f}")
        print(f"ğŸ“‰ RMSE (ì œê³± í‰ê·  ì˜¤ì°¨): {rmse:.2f}")
        print(f"ğŸ“ˆ RÂ² Score (ê²°ì •ê³„ìˆ˜): {r2:.4f}")

# ğŸ”¥ ë©”ì¸ ì‹¤í–‰ë¶€ ì¶”ê°€ (ë°˜ë“œì‹œ íŒŒì¼ ëì—!)
if __name__ == "__main__":
    print("ğŸ”¥ FILE EXECUTED\n")

    INTERVALS = {
        '5m': '5',
        '15m': '15',
        '1h': '60',
        '4h': '240'
    }

    for name, bybit_code in INTERVALS.items():
        train_and_save_model(name, bybit_code)
